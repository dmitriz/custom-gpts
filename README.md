# Custom GPTs Evaluation Repository

Evaluation framework for [OpenAI Custom GPTs](https://openai.com/index/introducing-gpts/). Test whether your Custom GPTs actually follow their instructions compared to regular ChatGPT.

**Focus:** OpenAI's "GPTs" feature (ChatGPT Plus/Enterprise), not third-party services.

## 📁 Current Structure

- **📝 `simple-evaluation.md`** - Minimal evaluation method
- **📋 `PLANNING.md`** - Project roadmap and workflow
- **📂 `advanced/`** - Complex features for future development
- **📂 `rulebooks/`** - Reusable rule sets and behavioral protocols that define the core logic for working with and customizing GPTs. Each file is designed to be modular, minimal, and composable — such as `universal-core.md`, which defines foundational behavioral standards and automation protocols for any GPT configuration.

## 🎯 Goal

Create the simplest possible method to compare Custom GPT vs regular ChatGPT performance.

## 🔄 Workflow Rules

- HIGH-IMPACT changes only
- NO implementation without explicit approval
- Focus on minimalism and simplicity
