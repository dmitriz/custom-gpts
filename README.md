# Custom GPTs Evaluation Repository

Evaluation framework for [OpenAI Custom GPTs](https://openai.com/index/introducing-gpts/). Test whether your Custom GPTs actually follow their instructions compared to regular ChatGPT.

**Focus:** OpenAI's "GPTs" feature (ChatGPT Plus/Enterprise), not third-party services.

## ğŸ“ Current Structure

- **ğŸ“ `simple-evaluation.md`** - Minimal evaluation method
- **ğŸ“‹ `PLANNING.md`** - Project roadmap and workflow
- **ğŸ“‚ `advanced/`** - Complex features for future development
- **ğŸ“‚ `rulebooks/`** - Reusable rule sets and behavioral protocols that define the core logic for working with and customizing GPTs. Each file is designed to be modular, minimal, and composable â€” such as `universal-core.md`, which defines foundational behavioral standards and automation protocols for any GPT configuration.

## ğŸ¯ Goal

Create the simplest possible method to compare Custom GPT vs regular ChatGPT performance.

## ğŸ”„ Workflow Rules

- HIGH-IMPACT changes only
- NO implementation without explicit approval
- Focus on minimalism and simplicity
