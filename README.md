# Custom GPTs Evaluation Repository

Evaluation framework for [OpenAI Custom GPTs](https://openai.com/index/introducing-gpts/). Test whether your Custom GPTs actually follow their instructions compared to regular ChatGPT.

**Focus:** OpenAI's "GPTs" feature (ChatGPT Plus/Enterprise), not third-party services.

## ğŸ“ Current Structure

- **ğŸ“ `simple-evaluation.md`** - Minimal evaluation method
- **ğŸ“‹ `PLANNING.md`** - Project roadmap and workflow
- **ğŸ“‚ `advanced/`** - Complex features for future development
- **ğŸ“‚ `rulebooks/`** - Reusable, modular, minimal, and composable behavioral protocols that define the core logic for working with and customizing GPTs.
  - **ğŸ“„ [`universal-core.md`](rulebooks/universal-core.md)** - Foundational behavioral standards and automation protocols included in all Custom GPT configurations.

## ğŸ¯ Goal

Create the simplest possible method to compare Custom GPT vs regular ChatGPT performance.

## ğŸ”„ Workflow Rules

- HIGH-IMPACT changes only
- NO implementation without explicit approval
- Focus on minimalism and simplicity
