# Custom GPTs Testing Repository

Testing and evaluation framework for OpenAI Custom GPTs. This repository contains tools for evaluating AI response quality using structured rulebooks and test cases.

**Important:** This project focuses on OpenAI's "GPTs" feature (ChatGPT Plus/Enterprise), not third-party services.

## Current Repository Structure

## ğŸ“ Current Repository Structure

### âœ… Implemented Components

- **ğŸ“œ `rulebooks/rules.md`** - Evaluation criteria and scoring framework ([see evaluation rules](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_format_inputs_to_ChatGPT_models.md))
- **ğŸ¯ `gpt-evaluator-simple.md`** - Simplified GPT configuration for initial testing  
- **ğŸ“Š `evaluations/test-case-1.md`** - Sample API design evaluation scenario
- **ğŸ“‹ `PLANNING.md`** - Future roadmap and workflow rules

### ğŸ“‚ Directory Details

- **`rulebooks/`** - Structured evaluation criteria and scoring rubrics for AI response quality
- **`evaluations/`** - Test scenarios and evaluation cases (not software tests - AI response evaluations)  
- **`prompts/`** - Complex GPT instructions (moved to backlog - too advanced for initial testing)
- **`.github/`** - Project workflow rules and Copilot instructions

### Workflow Rules

- HIGH-IMPACT changes only
- NO implementation without approval
- SHORT responses preferred
- NO examples without approval (bias concern)

## Current Status

- âœ… Scoring system complete (1-5 scale defined)
- âœ… Basic test case available  
- â³ Additional test topics needed
- â³ Edge case testing planned
