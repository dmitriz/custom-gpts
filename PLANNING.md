# Custom GPT Testing Project - Planning & Ideas

## Project Goal

Testing OpenAI Custom GPTs for professional-level AI assistance. 

**Important:** This refers specifically to OpenAI's "GPTs" feature in ChatGPT Plus/Enterprise (see [OpenAI GPTs documentation](https://openai.com/index/introducing-gpts/)), NOT third-party services using similar names. Focus on quality evaluation and best practices for OpenAI's platform only.

## Workflow Rules (Strict Adherence Required)

- **NO implementation without explicit approval**
- **HIGH-IMPACT, QUICK WINS ONLY** - everything else goes to planning
- **SHORT responses only** - no long explanations
- **NO examples unless approved** - examples can confuse/bias
- **CRITICAL review only** - discard minor items

## ðŸŽ¯ Project Intent

**Main Goal:** Evaluate Custom GPTs created on OpenAI platform to see if they actually follow their custom instructions.

**Specific Use Case:** Compare responses between:
- Your Custom GPT (with specific instructions)  
- Regular ChatGPT (no custom instructions)

**Key Question:** Does the Custom GPT perform differently/better than regular ChatGPT according to its instructions?

## âš¡ Current Simple Focus

- Create minimal evaluation method
- Test one Custom GPT vs regular ChatGPT
- Document clear differences (or lack thereof)

## ðŸ”„ Advanced Features (Future Work)

All complex features moved to `advanced/` folder:
- Multi-criteria scoring systems
- Detailed evaluation rubrics  
- Complex test scenarios

## Future Ideas (Capture Only - No Implementation)

- Expand test coverage to different technical topics (software architecture, security, performance)
- Add edge case testing for GPT responses (ambiguous inputs, conflicting requirements, incomplete information)
- Research additional MCP servers for enhanced capabilities
- Best practices documentation for Custom GPT creation and evaluation
- Sustainable project structure for scaling test cases

## Notes

- Focus on most urgent items only
- Using online pull request reviewers for feedback
- Concern about examples causing confusion/bias in evaluation
