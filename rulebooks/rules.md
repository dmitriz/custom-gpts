# Evaluation Rulebook for Software/API Response Comparison

## Rule 1: Evidence-Based Reasoning

Responses must include clear references to factual, verifiable, or well-established software principles, standards, or documentation.

## Rule 2: No Hidden Assumptions

Every assumption must be declared explicitly. If the GPT assumes anything, it must explain why and on what basis.

## Rule 3: Clarity and Structure

Responses must be logically structured and clearly written, using bullet points or headings when helpful.

## Rule 4: Depth of Technical Insight

Superficial answers score low. Depth in architecture, design trade-offs, limitations, and real-world implications is valued.

## Rule 5: Critical Evaluation

Responses should highlight flaws or gaps in logic or knowledge, and propose actionable improvements when needed.

## Rule 6: Single-Version Policy for Research Files

- Only one version of each research file is allowed. Always update the main file (e.g., research-findings-2025.md) and delete all duplicates immediately. No parallel or backup versions.

## Scoring Criteria (per response)

- Accuracy: __/5
- Depth: __/5
- Clarity: __/5
- Rule Adherence: __/5
- Overall Usefulness: __/5
